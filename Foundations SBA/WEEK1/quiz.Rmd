---
title: "Week 1: Quiz - Module 2"
author: "Kerry Sorenson"
date: "April 4, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r PastaData,message=FALSE,echo=FALSE}

require(data.table)
require(tidyr)
require(ggplot2)


dt1 <-  fread('DATA_2.01_SKU.csv',
                header = TRUE, 
                nrows=-1, 
                stringsAsFactors = FALSE, 
                na.strings='')
str(dt1)
```

#### 1. What is the correct mean and median of the coefficient of variations of the sales in the SKU dataset (DATA_2.01_SKU.csv)?

```{r, echo=FALSE}
summary(dt1$CV)
```

#### 2. Do a hierarchical clustering on scaled data using an Euclidian distance and Ward.D clustering on the SKU dataset (DATA_2.01_SKU.csv). What are the resulting segments compared to what is shown in class if you decide to take only 2 clusters?

```{r, echo=FALSE, message=FALSE}
data = dt1

# Let's find groups using hierarchical clustering and check if we obtain similar results
testdata=data  # To keep our dataset safe, let's create a copy of it called "testdata"
testdata = scale(testdata) # To keep our dataset safe, let's create a copy of it called "testdata"

d = dist(testdata, method = "euclidean") # the dist() function computes the distances of all the observations in our dataset
hcward = hclust(d, method="ward.D") # hclust() function performs hiearchical clustering, we pass it the distances, and we set the method argument to "ward.D"

data$groups<-cutree(hcward,k=2) # assign our points to our k=3 clusters 

# The lattice library provides a complete set of functions for producing advanced plots.
#install.packages("lattice") #install the lattice package by using the install.packages() function
require(lattice) # load the lattice package by using the library() function and passing it the name of the package you wish to load
xyplot(ADS~ CV,main = "After Clustering", type="p",group=groups,data=data, # define the groups to be differentiated 
       auto.key=list(title="Group", space = "left", cex=1.0, just = 0.95), # to produce the legend we use the auto.key= list() 
       par.settings = list(superpose.line=list(pch = 0:18, cex=1)), # the par.settings argument allows us to pass a list of display settings
       col=c('blue','green','red')) # finally we choose the colour of our plotted points per group
print('Answer: The segments "Crickets" and "Wild Bulls" are merged')
```

#### 3. Which of the following graphs reports the correct plot of the last project evaluation as a function of the number of projects done for the HR dataset (DATA_2.02_HR.csv)?

```{r, echo=FALSE, message=FALSE}
dt2 <-  fread('DATA_2.02_HR.csv',
                header = TRUE, 
                nrows=-1, 
                stringsAsFactors = FALSE, 
                na.strings='')
ggplot(data=dt2,aes(x=LPE,y=NP)) + geom_point()

```

#### 4. If you cluster the HR dataset (DATA_2.02_HR.csv) on Satisfaction, Project Evaluation and Number of Projects Done and that you keep 2 segments using the same values for the other specifications (scaling, distance type and clustering algorithm), what's the resulting median Satisfaction per segment?

```{r, echo=FALSE, message=FALSE}
data = dt2
# As discussed in the videos, let's remove the Newborn variable, which is not really relevant and by being a dummy drives the clustering too much...
testdata=data[,.(S,LPE,NP)] # we create a new dataset, called "testdata" includes all the rows and the 5 first columns of our original dataset 

# We then rerun the code used above
testdata = scale(testdata) # We normalize again our original variables
d = dist(testdata, method = "euclidean") # We compute the distances between observations
hcward = hclust(d, method="ward.D") # Hiearchical Clustering using Ward criterion

data$groups = cutree(hcward,k=2) # Create segments for k=4
# Note that we re-use the original dataset "data" (where the variable Newborn is still present) and not "testdata" (where the variable Newborn has been removed)
# Hence we'll be able to produce summary statistics also for the Newborn variable regardless it wasn't included when doing the second version of the clustering

aggregate(S~groups, data=data, FUN=median) # Aggregate the values again

```

#### 5. For the Telecom dataset (DATA_2.03_Telco.csv), using the specifications of the example presented in the videos, which of the following claim is correct?

```{r, echo=FALSE, message=FALSE}
dt3 <-  fread('DATA_2.03_Telco.csv',
                header = TRUE, 
                nrows=-1, 
                stringsAsFactors = FALSE, 
                na.strings='')
data = dt3

# Now let's normalize our variables
testdata=data # To keep our dataset safe, let's create a copy of it called "testdata"
testdata = scale(testdata) # the scale function automatically performs data normalization on all your variables

d = dist(testdata, method = "euclidean") # the dist() function computes the distances of all the observations in our dataset
hcward = hclust(d, method="ward.D") # hclust() function performs hiearchical clustering, we pass it the distances, and we set the method argument to "ward.D"

# Let's try again with 5 segments
data$groups= cutree(hcward,k=5) #Create segments for k=5
aggdata= aggregate(.~ groups, data=data, FUN=mean) # Aggregation by group and computation of the mean values
proptemp=aggregate(Calls~ groups, data=data, FUN=length) # Computation of the number of observations by group
aggdata$proportion=(proptemp$Calls)/sum(proptemp$Calls) # Computation of the proportion by group
aggdata=aggdata[order(aggdata$proportion,decreasing=T),] # Ordering from the largest group to the smallest

aggdata$Label <- c('Heavy User','YA','Silver','Pro','Light User')

aggdata[,c(8,2:7)]

summary(dt3[,.(Intern)])

```



